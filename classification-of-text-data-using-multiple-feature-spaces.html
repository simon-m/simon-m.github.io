
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="/themes/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="/themes/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="/themes/font-awesome/css/font-awesome.min.css">


    <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Weblog Atom">




  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />


<meta name="author" content="Simon-M" />
<meta name="description" content="How to combine multiple topic-based and word embedding-based methods for text classification with scikit-learn and Gensim." />
<meta name="keywords" content="NLP, Text classification, Word embedding, Python, Scikit-learn, Sklearn, Gensim">

<meta property="og:site_name" content="Weblog"/>
<meta property="og:title" content="Classification of text data: using multiple feature spaces"/>
<meta property="og:description" content="How to combine multiple topic-based and word embedding-based methods for text classification with scikit-learn and Gensim."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/classification-of-text-data-using-multiple-feature-spaces.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-11-02 21:40:00+01:00"/>
<meta property="article:modified_time" content="2017-11-02 21:40:00+01:00"/>
<meta property="article:author" content="/author/simon-m.html">
<meta property="article:section" content="Machine learning"/>
<meta property="article:tag" content="NLP"/>
<meta property="article:tag" content="Text classification"/>
<meta property="article:tag" content="Word embedding"/>
<meta property="article:tag" content="Python"/>
<meta property="article:tag" content="Scikit-learn"/>
<meta property="article:tag" content="Sklearn"/>
<meta property="article:tag" content="Gensim"/>
<meta property="og:image" content="">

  <title>Weblog &ndash; Classification of text data: using multiple feature spaces</title>


  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>
<body>
  <aside>
    <div>
      <a href="">
        <img src="/themes/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>


      <nav>
        <ul class="list">
          <li><a href="/pages/links.html#links">Links</a></li>

        </ul>
      </nav>

      <ul class="social">
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="">    Home
</a>

      <a href="categories.html">Categories</a>
      <a href="tags.html">Tags</a>

      <a href="/feeds/all.atom.xml">    Atom
</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="classification-of-text-data-using-multiple-feature-spaces">Classification of text data: using multiple feature spaces</h1>
    <p>
          Posted on jeu. 02 novembre 2017 in <a href="/category/machine-learning.html">Machine learning</a>


    </p>
  </header>


  <div>
    <p>As mentionned in another <a href="using-facebooks-fasttext-for-document-classification.html">post</a>, I am currently working on a text classification task and experimenting with
several features extraction methods.</p>
<h1>Input features for text classification</h1>
<h2>Topic-based</h2>
<p>I have started with the regular "topic-based" method such as
<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">latent semantic indexing/analysis</a> (LSI/LSA),
<a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">latent dirichlet allocation</a> (LDA)
and <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">non-negative matrix factorization</a> (NMF).</p>
<p>These methods start with a term-document matrix \(T\) with documents as rows and terms as columns.
In the simplest case, the value \(T_{ij}\) is simply the absolute frequency (or count) of
term \(j\) in document \(i\). This value is often replaced by the so-called TF-IDF value (Term Frequency -
Inverse Document Frequency) which basically allows to give more importance to rare terms.
Note that "terms" could be words or n-grams or possibly any other relevant unit of text.</p>
<p>From this matrix, topic-based methods seek to discover latent factors called <em>topics</em>, which are linear
combinations of the terms and represent documents as linear combinations of topics.
The expectation is that words found in similar documents will end up in the same topic,
hoping that topics are more releveant than bare words for classification.
Moreover using \(T\) directly as input to a classifier would result in on feature per word which can
lead to a prohibitively large number of features. Topic extraction can therefore be seen as
a dimensionality reduction step.</p>
<p>In practice LSI uses singular value decomposition (SVD) decomposition on \(T\),
LDA is a probabilistic model over topics and documents, and NMF, well, relies on the
non-negative matrix factorization of \(T\).</p>
<h2>Word embedding-based</h2>
<p>Although these approaches are standard in text analysis, I was curious about the newer so-called <em>word embedding</em>
methods such as <a href="Bag of Tricks for Efficient Text Classification">Facebook's FastText</a>. These follow a rather
orthogonal approach to topic-based methods as they
seek to find a vector representation of <em>words</em> so that semantically similar words are represented by similar vectors
(according to a given metric).
To reach this goal, the broad idea is to find an embedding allowing to predict which word should occur given its <em>context</em>
(for the continuous bag of words representation, the skip-gram representation swaps words and contexts).
Here context mean "surroundings words", i.e. words found in a windows around the word of interest.
Note that I use "word" instead of "terms", but this can also be applied to n-grams as a unit.</p>
<p>As opposed to topic-based methods, word-embedding methods consider a more local context:
for the former, similar terms are those appearing in similar documents, for the latter,
similar terms appear in similar contexts <em>within</em> a document.</p>
<h2>The best of both worlds?</h2>
<p>The two approaches seeming quite complementary so I thought may give a shot to
combining their resulting features. I settled to use NMF and a FastText-based
document embedding.</p>
<h1>In practice: document FastText</h1>
<p>Document embedding methods results in vectors for <em>terms</em> but not for documents.
Therefore I used a fairly simple method to get document-vectors from terms-vectors: simply
concatenate the element-wise min, max and mean of all words in the document. For a
term-embedding of size \(k\), this results in a document-embedding of size \(3k\).
This original idea was described <a href="https://arxiv.org/abs/1607.00570">here</a> and gave simingly good
results for short documents.</p>
<p>The code using Gensim's FastText:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">gensim.models.fasttext</span> <span class="kn">import</span> <span class="n">FastText</span>


<span class="k">class</span> <span class="nc">DocumentFastText</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">max_vocab_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">word_ngrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ns&#39;</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">min_alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hashfxn</span><span class="o">=</span><span class="nb">hash</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">null_word</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">min_n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_n</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">sorted_vocab</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bucket</span><span class="o">=</span><span class="mi">2000000</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">batch_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg</span> <span class="o">=</span> <span class="n">sg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hs</span> <span class="o">=</span> <span class="n">hs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="n">window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span> <span class="o">=</span> <span class="n">max_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_ngrams</span> <span class="o">=</span> <span class="n">word_ngrams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="n">workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">=</span> <span class="n">min_alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">=</span> <span class="n">negative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="o">=</span> <span class="n">cbow_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hashfxn</span> <span class="o">=</span> <span class="n">hashfxn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="nb">iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">null_word</span> <span class="o">=</span> <span class="n">null_word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_n</span> <span class="o">=</span> <span class="n">min_n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_n</span> <span class="o">=</span> <span class="n">max_n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sorted_vocab</span> <span class="o">=</span> <span class="n">sorted_vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket</span> <span class="o">=</span> <span class="n">bucket</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trim_rule</span> <span class="o">=</span> <span class="n">trim_rule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_words</span> <span class="o">=</span> <span class="n">batch_words</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast_text</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">sg</span><span class="p">,</span> <span class="n">hs</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">max_vocab_size</span><span class="p">,</span>
                 <span class="n">word_ngrams</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">workers</span><span class="p">,</span> <span class="n">min_alpha</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="p">,</span>
                 <span class="n">hashfxn</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">null_word</span><span class="p">,</span> <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span><span class="p">,</span> <span class="n">sorted_vocab</span><span class="p">,</span>
                 <span class="n">bucket</span><span class="p">,</span> <span class="n">trim_rule</span><span class="p">,</span> <span class="n">batch_words</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast_text</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast_text</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>


<h1>In practice: getting the inputs right</h1>
<p>Now, my workflow is based on scikit-learn's
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline">pipelines</a>
and FastText has been implemented in the Gensim Library but not in sklearn as it is not general enough.
for this reason, FastText was not design to work with sklearn's convenient
<a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text">Vectorizers</a>
and has to be fed with a list of words instead of a document-term matrix.
Thus I have to find a way to:</p>
<ol>
<li>
<p>combine the feature coming from both methods before feeding them to the classifier which can be done easily with a FeatureUnion</p>
</li>
<li>
<p>do so starting from a different representation of the text (list versus document-term matrix)</p>
</li>
<li>
<p>allow some parameters to be shared between these representations (stop words for instance)</p>
</li>
</ol>
<p>For this, let's first build a class which replicates the pre-processing and tokenizing steps of the Vectorizer.
This yields a list a words for FastText to use while still taking into account the parameters passed to the original
vectorizer and is used with NMF.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span>

<span class="k">class</span> <span class="nc">TextPreProcessor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">vectorizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">build_preprocessor</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">build_tokenizer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">text</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>


<h1>In practice: putting it all together</h1>
<p>Then, it is mostly a matter of building and pluging the corresponding pipes together
into a final pipeline:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">FeatureUnion</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span><span class="p">,</span> <span class="n">GridSearchCV</span>

<span class="n">train_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">doc_fast_text</span> <span class="o">=</span> <span class="n">pftc</span><span class="o">.</span><span class="n">DocumentFastText</span><span class="p">()</span>
<span class="n">fasttext_subpipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;text_prepro&#39;</span><span class="p">,</span> <span class="n">pftc</span><span class="o">.</span><span class="n">TextPreProcessor</span><span class="p">(</span><span class="n">train_vectorizer</span><span class="p">)),</span>
                                   <span class="p">(</span><span class="s1">&#39;transfo&#39;</span><span class="p">,</span> <span class="n">doc_fast_text</span><span class="p">)])</span>
<span class="n">nmf</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">()</span>
<span class="n">nmf_subpipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;vectorizer&#39;</span><span class="p">,</span> <span class="n">train_vectorizer</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;transfo&#39;</span><span class="p">,</span> <span class="n">nmf</span><span class="p">)])</span>

<span class="n">feature_union</span> <span class="o">=</span> <span class="n">FeatureUnion</span><span class="p">([(</span><span class="s2">&quot;embedding&quot;</span><span class="p">,</span> <span class="n">fasttext_subpipe</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;topics&quot;</span><span class="p">,</span> <span class="n">nmf_subpipe</span><span class="p">)])</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">()</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;feature_extraction&#39;</span><span class="p">,</span> <span class="n">feature_union</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;classfier&quot;</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)])</span>
</pre></div>


<p>I can then run my workflow; a grid search over parameters for instance.
Conveniently, multiple nesting of parameters are handled with the "__" syntax.</p>
<div class="highlight"><pre><span></span><span class="n">params_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;feature_extraction__embedding__transfo__size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="s2">&quot;feature_extraction__embedding__transfo__min_count&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="s2">&quot;feature_extraction__embedding__transfo__word_ngrams&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>

    <span class="s2">&quot;feature_extraction__topics__vectorizer__ngram_range&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
    <span class="s2">&quot;feature_extraction__topics__vectorizer__binary&quot;</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>

    <span class="s2">&quot;feature_extraction__topics__transfo__n_components&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="s2">&quot;feature_extraction__topics__transfo__alpha&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="s2">&quot;feature_extraction__topics__transfo__l1_ratio&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">kfold_cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">()</span>
<span class="n">gs_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params_grid</span><span class="p">,</span>
                     <span class="n">scoring</span><span class="o">=</span><span class="n">best_cut_mcc_scoring</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold_cv</span><span class="p">,</span>
                     <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_cores</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                     <span class="n">random_state</span><span class="o">=</span><span class="n">RANDOM_STATE_SEED</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_text</span><span class="p">,</span> <span class="n">train_decision</span><span class="p">)</span>
</pre></div>


<p>And that's it.</p>
<h1>Epilogue</h1>
<p>It turns out that using both feature spaces as input improves on using either separately.</p>
<h1>References:</h1>
<ul>
<li><a href="https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector">https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector</a></li>
<li><a href="https://arxiv.org/abs/1607.00570">https://arxiv.org/abs/1607.00570</a></li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/nlp.html">NLP</a>
      <a href="/tag/text-classification.html">Text classification</a>
      <a href="/tag/word-embedding.html">Word embedding</a>
      <a href="/tag/python.html">Python</a>
      <a href="/tag/scikit-learn.html">Scikit-learn</a>
      <a href="/tag/sklearn.html">Sklearn</a>
      <a href="/tag/gensim.html">Gensim</a>
    </p>
  </div>



    <div class="addthis_relatedposts_inline">


</article>

    <footer>
<p>&copy;  </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Weblog ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>

</body>
</html>