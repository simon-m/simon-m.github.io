<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Weblog - Machine learning</title><link href="https://simon-m.github.io/" rel="alternate"></link><link href="https://simon-m.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://simon-m.github.io/</id><updated>2017-11-25T18:55:00+01:00</updated><entry><title>Classification metrics</title><link href="https://simon-m.github.io/classification-metrics.html" rel="alternate"></link><published>2017-11-25T18:55:00+01:00</published><updated>2017-11-25T18:55:00+01:00</updated><author><name>Simon-M</name></author><id>tag:simon-m.github.io,2017-11-25:/classification-metrics.html</id><summary type="html">&lt;p&gt;A sample of the existing classification metrics with their interpretation.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I never seem to remember these basic measures so I made a quick reference.
Of course this is a very incomplete list. A very nice reference is wikipedia's
page about the &lt;a href="https://en.wikipedia.org/wiki/Confusion_matrix"&gt;confusion matrix&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let \(P\) and \(N\) represent the real positive and negatives and
\(\tilde{P}\) and \(\tilde{N}\) be the predicted positive and negatives.
The following confusion matrix will be our reference:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;\(P\)&lt;/th&gt;
&lt;th align="center"&gt;\(N\)&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;\( \tilde P \)&lt;/td&gt;
&lt;td align="center"&gt;TP = a&lt;/td&gt;
&lt;td align="center"&gt;FP = b&lt;/td&gt;
&lt;td&gt;= a + b&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;\( \tilde N \)&lt;/td&gt;
&lt;td align="center"&gt;FN = c&lt;/td&gt;
&lt;td align="center"&gt;TN = d&lt;/td&gt;
&lt;td&gt;= c + d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align="center"&gt;= a + c&lt;/td&gt;
&lt;td align="center"&gt;= b + d&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Sensitivity, true positive rate, recall, probability of detection:
\[Sens = \frac{TP}{P} = \frac{a}{a+c}\]
Probability that positive tests are rightly so: \(\mathcal{P}(\tilde{P} | P)\).
Intrinsic to the test / classifier.&lt;/p&gt;
&lt;p&gt;Specificity, true negative rate:
\[Sens = \frac{TN}{N} = \frac{d}{b+d}\]
Probability that negative tests are rightly so : \(\mathcal{P}(\tilde{N} | N)\).
Intrinsic to the test / classifier.&lt;/p&gt;
&lt;p&gt;Positive predictive value, precision:
\[Sens = \frac{TP}{\tilde{P}} = \frac{a}{a+b}\]
Probability that, given a postitive prediction, it is a real positive:
\(\mathcal{P}(P | \tilde{P})\).
Not intrinsic to the test / classifier; also depends on \(P\).&lt;/p&gt;
&lt;p&gt;Negative predictive value, precision (note: depends on \(P\)):
\[Sens = \frac{TN}{\tilde{N}} = \frac{d}{c+d}\]
Probability that, given a negative prediction, it is a real negative:
\(\mathcal{P}(N | \tilde{N})\).
Not intrinsic to the test / classifier; also depends on \(P\).&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity"&gt;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values"&gt;https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Confusion_matrix"&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine learning"></category><category term="Classification metrics"></category><category term="Precision"></category><category term="Sensitivity"></category><category term="Confusion matrix"></category><category term="Predictive value"></category></entry><entry><title>Classification of text data: using multiple feature spaces</title><link href="https://simon-m.github.io/classification-of-text-data-using-multiple-feature-spaces.html" rel="alternate"></link><published>2017-11-02T21:40:00+01:00</published><updated>2017-11-02T21:40:00+01:00</updated><author><name>Simon-M</name></author><id>tag:simon-m.github.io,2017-11-02:/classification-of-text-data-using-multiple-feature-spaces.html</id><summary type="html">&lt;p&gt;How to combine multiple topic-based and word embedding-based methods for text classification with scikit-learn and Gensim.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As mentionned in another &lt;a href="using-facebooks-fasttext-for-document-classification.html"&gt;post&lt;/a&gt;, I am currently working on a text classification task and experimenting with
several features extraction methods.&lt;/p&gt;
&lt;h1&gt;Input features for text classification&lt;/h1&gt;
&lt;h2&gt;Topic-based&lt;/h2&gt;
&lt;p&gt;I have started with the regular "topic-based" method such as
&lt;a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"&gt;latent semantic indexing/analysis&lt;/a&gt; (LSI/LSA),
&lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;latent dirichlet allocation&lt;/a&gt; (LDA)
and &lt;a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"&gt;non-negative matrix factorization&lt;/a&gt; (NMF).&lt;/p&gt;
&lt;p&gt;These methods start with a term-document matrix \(T\) with documents as rows and terms as columns.
In the simplest case, the value \(T_{ij}\) is simply the absolute frequency (or count) of
term \(j\) in document \(i\). This value is often replaced by the so-called TF-IDF value (Term Frequency -
Inverse Document Frequency) which basically allows to give more importance to rare terms.
Note that "terms" could be words or n-grams or possibly any other relevant unit of text.&lt;/p&gt;
&lt;p&gt;From this matrix, topic-based methods seek to discover latent factors called &lt;em&gt;topics&lt;/em&gt;, which are linear
combinations of the terms and represent documents as linear combinations of topics.
The expectation is that words found in similar documents will end up in the same topic,
hoping that topics are more releveant than bare words for classification.
Moreover using \(T\) directly as input to a classifier would result in on feature per word which can
lead to a prohibitively large number of features. Topic extraction can therefore be seen as
a dimensionality reduction step.&lt;/p&gt;
&lt;p&gt;In practice LSI uses singular value decomposition (SVD) decomposition on \(T\),
LDA is a probabilistic model over topics and documents, and NMF, well, relies on the
non-negative matrix factorization of \(T\).&lt;/p&gt;
&lt;h2&gt;Word embedding-based&lt;/h2&gt;
&lt;p&gt;Although these approaches are standard in text analysis, I was curious about the newer so-called &lt;em&gt;word embedding&lt;/em&gt;
methods such as &lt;a href="Bag of Tricks for Efficient Text Classification"&gt;Facebook's FastText&lt;/a&gt;. These follow a rather
orthogonal approach to topic-based methods as they
seek to find a vector representation of &lt;em&gt;words&lt;/em&gt; so that semantically similar words are represented by similar vectors
(according to a given metric).
To reach this goal, the broad idea is to find an embedding allowing to predict which word should occur given its &lt;em&gt;context&lt;/em&gt;
(for the continuous bag of words representation, the skip-gram representation swaps words and contexts).
Here context mean "surroundings words", i.e. words found in a windows around the word of interest.
Note that I use "word" instead of "terms", but this can also be applied to n-grams as a unit.&lt;/p&gt;
&lt;p&gt;As opposed to topic-based methods, word-embedding methods consider a more local context:
for the former, similar terms are those appearing in similar documents, for the latter,
similar terms appear in similar contexts &lt;em&gt;within&lt;/em&gt; a document.&lt;/p&gt;
&lt;h2&gt;The best of both worlds?&lt;/h2&gt;
&lt;p&gt;The two approaches seeming quite complementary so I thought may give a shot to
combining their resulting features. I settled to use NMF and a FastText-based
document embedding.&lt;/p&gt;
&lt;h1&gt;In practice: document FastText&lt;/h1&gt;
&lt;p&gt;Document embedding methods results in vectors for &lt;em&gt;terms&lt;/em&gt; but not for documents.
Therefore I used a fairly simple method to get document-vectors from terms-vectors: simply
concatenate the element-wise min, max and mean of all words in the document. For a
term-embedding of size \(k\), this results in a document-embedding of size \(3k\).
This original idea was described &lt;a href="https://arxiv.org/abs/1607.00570"&gt;here&lt;/a&gt; and gave simingly good
results for short documents.&lt;/p&gt;
&lt;p&gt;The code using Gensim's FastText:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.base&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseEstimator&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gensim.models.fasttext&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;FastText&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DocumentFastText&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.025&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;max_vocab_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;word_ngrams&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ns&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;min_alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cbow_mean&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hashfxn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;null_word&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;min_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_vocab&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bucket&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2000000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trim_rule&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;batch_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;sentences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sg&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;min_count&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_vocab_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_vocab_size&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_ngrams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_ngrams&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;workers&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min_alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;min_alpha&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;negative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;negative&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cbow_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbow_mean&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hashfxn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hashfxn&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;null_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;null_word&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;min_n&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max_n&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sorted_vocab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted_vocab&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bucket&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bucket&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trim_rule&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trim_rule&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;batch_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch_words&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fast_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FastText&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;word_ngrams&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;negative&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cbow_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;hashfxn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;null_word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorted_vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;bucket&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trim_rule&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fast_text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_vocab&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fast_text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;In practice: getting the inputs right&lt;/h1&gt;
&lt;p&gt;Now, my workflow is based on scikit-learn's
&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline"&gt;pipelines&lt;/a&gt;
and FastText has been implemented in the Gensim Library but not in sklearn as it is not general enough.
for this reason, FastText was not design to work with sklearn's convenient
&lt;a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"&gt;Vectorizers&lt;/a&gt;
and has to be fed with a list of words instead of a document-term matrix.
Thus I have to find a way to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;combine the feature coming from both methods before feeding them to the classifier which can be done easily with a FeatureUnion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;do so starting from a different representation of the text (list versus document-term matrix)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;allow some parameters to be shared between these representations (stop words for instance)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, let's first build a class which replicates the pre-processing and tokenizing steps of the Vectorizer.
This yields a list a words for FastText to use while still taking into account the parameters passed to the original
vectorizer and is used with NMF.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.base&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseEstimator&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TextPreProcessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preprocess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_preprocessor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_tokenizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;preprocess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;In practice: putting it all together&lt;/h1&gt;
&lt;p&gt;Then, it is mostly a matter of building and pluging the corresponding pipes together
into a final pipeline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;NMF&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;

&lt;span class="n"&gt;train_vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;doc_fast_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pftc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DocumentFastText&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;fasttext_subpipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;text_prepro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pftc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TextPreProcessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_vectorizer&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                                   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;transfo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;doc_fast_text&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;nmf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NMF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;nmf_subpipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;vectorizer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_vectorizer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;transfo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nmf&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="n"&gt;feature_union&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;embedding&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fasttext_subpipe&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;topics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nmf_subpipe&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;feature_extraction&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_union&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;classfier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I can then run my workflow; a grid search over parameters for instance.
Conveniently, multiple nesting of parameters are handled with the "__" syntax.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;params_grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__embedding__transfo__size&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__embedding__transfo__min_count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__embedding__transfo__word_ngrams&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

    &lt;span class="s2"&gt;&amp;quot;feature_extraction__topics__vectorizer__ngram_range&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__topics__vectorizer__binary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

    &lt;span class="s2"&gt;&amp;quot;feature_extraction__topics__transfo__n_components&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__topics__transfo__alpha&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;feature_extraction__topics__transfo__l1_ratio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;kfold_cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;gs_cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param_grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;best_cut_mcc_scoring&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;kfold_cv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_cores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;RANDOM_STATE_SEED&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_decision&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And that's it.&lt;/p&gt;
&lt;h1&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;It turns out that using both feature spaces as input improves on using either separately.&lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector"&gt;https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.00570"&gt;https://arxiv.org/abs/1607.00570&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="NLP"></category><category term="Text classification"></category><category term="Word embedding"></category><category term="Python"></category><category term="Scikit-learn"></category><category term="Sklearn"></category><category term="Gensim"></category></entry><entry><title>Using Facebook's FastText for document classification.</title><link href="https://simon-m.github.io/using-facebooks-fasttext-for-document-classification.html" rel="alternate"></link><published>2017-10-24T18:30:00+02:00</published><updated>2017-10-31T23:00:00+01:00</updated><author><name>Simon-M</name></author><id>tag:simon-m.github.io,2017-10-24:/using-facebooks-fasttext-for-document-classification.html</id><summary type="html">&lt;p&gt;A short introduction to using FastText for document classification&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Context&lt;/h1&gt;
&lt;p&gt;I am currently working on classifying some medical data in the form of examination notes,
the goal being to predict whether a patient has a pathology or not.
This text data has a few quirks compared to textbook examples of document classification:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Documents (one per patient) are short: less than 30 words on average&lt;/li&gt;
&lt;li&gt;The text is dirty&lt;/li&gt;
&lt;li&gt;many word contain typos, resulting in multiple tmers refering to the same thing&lt;/li&gt;
&lt;li&gt;many abbreviations are used and end up in conflicts&lt;/li&gt;
&lt;li&gt;Many words and mostly important ones are subject-specific jargon and thus cannot be recovered
   using general-purpose spelling correctors.&lt;/li&gt;
&lt;li&gt;The single binary label pathology/no_pathology is noisy because it is the result of partial manual
   annotation. more specifically, a subset of the dataset was selected for annotation based on a
   currated list of keywords (and their typo'd variations). The rest was labeled as "no_pathology".
   Moreover, the manual annotation was also carried out in a fairly limited time-frame which did not allow
   multiple pass on the same patients by several annotators.&lt;/li&gt;
&lt;li&gt;The dataset in very unbalanced since around 1.2% of the patient are part of the "pathology" class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the plus side, the dataset is rather large with ~600000 patients.&lt;/p&gt;
&lt;p&gt;On the minus side, I have access to a fairly limited amount of computing power (no GPU, intel i3 with 4 Gb RAM).
Therefore, even though I am fairly attracted to the shiny neural networks-based methods
(in particular convolutional, recurrent and long-short term memory NN which seem to enjoy a fair share
of success) there is absolutely no way I can run them. Also, cloud-based solutions such as Amazon's AWS are out of
the question, the first reason for that being that the data should not leave the lab.&lt;/p&gt;
&lt;h1&gt;Facebook's fastText&lt;/h1&gt;
&lt;p&gt;After trying a bunch of different methods which will be the subject of another article,
I was reading on language models, in particular n-grams and word embeddings/representations
such as word2vec and GloVe, hoping to find new ideas for extracting relevant features from
my documents' text.&lt;/p&gt;
&lt;p&gt;Both the n-gram and word representation are interesting in their own right but I am rather
impressed by the semantic properties of word2vec. However, word embedding methods are called
this way for a reason: the unit are words, which means each word gets vector representation.
I am therefore left with the follwing question: how to meaningfully combine these and get a document
representation?&lt;/p&gt;
&lt;p&gt;I then stumbled upon an interesting article:
&lt;a href="https://arxiv.org/abs/1607.01759"&gt;Bag of Tricks for Efficient Text Classification&lt;/a&gt;.
In short, the approach described by the authors consists in defining word (or n-gram) representation
&lt;em&gt;à la&lt;/em&gt; word2vec, simply average them to get a sentence representation and feed that to a linear
classifier.&lt;/p&gt;
&lt;p&gt;I like this approach for several reasons: it uses the apparently powerful word2vec framework,
it is conceptually simple, it compares favorably to baselines and more sophisticated models,
it is fast and in particular much much faster than the neural network-based models I cannot
afford, and finally , it is already implemented in the
&lt;a href="https://github.com/facebookresearch/fastText/"&gt;fastText library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am therefore going to try my luck and use fastText's classifier to gauge its ability to
classify my data. If it works well I will also try to use the sentence representations as
extra features along with the LDA topic mixtures I have used so far.&lt;/p&gt;
&lt;p&gt;Note that fastText is not limited to classification: it can be used in an unsupervised fashion
to get word and sentence embeddings.&lt;/p&gt;
&lt;h2&gt;FastText for Windows&lt;/h2&gt;
&lt;p&gt;First hurdle: fastText is designed to build under Unix-like systems.
that's a bummer as despite being a long-standing user and fan of Linux systems,
I am stuck with Windows at work.
Fortunately, someone may have felt the same and shared a
&lt;a href="http://cs.mcgill.ca/~mxia3/FastText-for-Windows/"&gt;Windows version&lt;/a&gt; of the library.
Thumbs up to you &lt;a href="http://cs.mcgill.ca/~mxia3"&gt;Meng Xuan Xia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After a quick and painless compilation with MS Visual C++ (my first time using it), I am
ready to start.&lt;/p&gt;
&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;I am mainly giving  short version of the &lt;a href="https://github.com/facebookresearch/fastText/"&gt;documentation&lt;/a&gt;
and &lt;a href="https://github.com/facebookresearch/fastText/blob/master/tutorials"&gt;tutorials&lt;/a&gt;, so you
will probably need to read them too.&lt;/p&gt;
&lt;p&gt;The command-line interface (CLI) is very simple to use and the input format is very minimalistic
and convenient (I am looking at you Weka and ARFF).&lt;/p&gt;
&lt;p&gt;The training set format is as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;This is an example sentence __label__example
This is one is not (yes it is though) __label__not_example
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Basically, each sentence/document is written as is on a single line and the labels are
prefixed with __label__ (this can be customized using the -label option on the CLI).
In this case, I simply encode the single binary label as two labels "__label__pathology" and
"__label__no_pathology".&lt;/p&gt;
&lt;p&gt;Training a default classifier is dead simple with the "supervised" command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fastText.exe supervised -input training_set.txt -output default_model
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The performance of the resulting model can then be investigated using the test command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fastText.exe test default_model.bin test_file.txt 1;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The "1" here is the default value for the number of top label to be used as the prediction.
In the general case, a value of "1" will give as prediction the best label only.
In my case, since I have a single binary label, using "1" is the way to go.
This prints the precision achieved on the test set.&lt;/p&gt;
&lt;p&gt;In this case, precision is not a good measure because of the very strong unbalance of the dataset.
Always predicting "no_pathology", one can expect precision and recall to reach ~98.8%.
Such a high baseline makes it difficult to assess whether this classifier is better than the
dummy "always predict no-pathology" one.&lt;/p&gt;
&lt;p&gt;Fortunately, I can rely on the "predict" command to get predictions and compute the confusion matrix
or any other metric such as Matthews Correlation Coefficient (MCC).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fastText.exe predict default_model.bin test_file.txt 1 &amp;gt; predictions.txt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Still, I need a more detailed account of the classifier's performance. Specifically,
I would like to get a maximum of true positives (TP) (don't we all?) while keeping the number of
false positives (FP) close to TP. This has to do with the way the
classifier will be used, namely screening and human validation of the positive predictions.
The goal is to give a reasonable number of screened patients to the validators.
I therefore need to have access to a probability-like number instead of 0 / 1 decisions in order
to vary the "positive" cutoff during subsequent analyses.&lt;/p&gt;
&lt;p&gt;For that, I use the "predict-prob" command of fasttext.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fastText.exe predict-proba default_model.bin test_file.txt 1 &amp;gt; class_probabilities.txt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Parameters&lt;/h1&gt;
&lt;p&gt;A short description of the parameters is provided in the documentation, but I will
expand a little here on the most important ones.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The "-loss" option allows you to choose the loss which is optimized for.&lt;/p&gt;
&lt;p&gt;The default value is "ns" for negative sampling. It is an approximation of the softmax loss
based on sampling "negative examples".&lt;/p&gt;
&lt;p&gt;Briefly and very roughly, the goal of word2vec is to find an embedding of words, i.e. a vector
representation for each word in a space of fixed dimension.
Given a &lt;em&gt;context&lt;/em&gt; i.e. words surrounding a word of interest (e.g. in a fixed size window around it),
a good embedding should satisfy the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the dot product between the vectors of a word and a context often found together should be large.&lt;/li&gt;
&lt;li&gt;this dot product must be small for words and contexts which rarely co-occur&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given the vector representation \(w_i\) of a word and a context \(c_j\), the goal is
therefore to maximize \(\frac{w_i \cdot c_j}{sum_w(w \cdot c_j)}\). The normalization
is a sum over all words which can be prohibitively large and yield very long computation
times.&lt;/p&gt;
&lt;p&gt;Remark: this is a gross simplification; the actual softmax loss is actually
\(-log P(w_i | c_j) = -log(e^{w_i \cdot c_j} / sum_w(e^{w \cdot c_i}))\) which
should be minimized instead of maximized.&lt;/p&gt;
&lt;p&gt;An alternative is therefore to replace the sum over all words by  a sum over a random
sample, under the reasonable assumption that most are "negative", i.e. rarely co-occur
with a given context. This resutls in the negative sampling loss. See also this
&lt;a href="https://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term"&gt;stackoverflow post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The "softmax" value results in using the regular softmax loss which is much &lt;em&gt;much&lt;/em&gt; slower.&lt;/p&gt;
&lt;p&gt;The "hs" value uses hierarchical softmax. The goal is to organize the set of all words in a tree so that
the sum over all words reduces instrad to a sum over one path in the tree.&lt;/p&gt;
&lt;p&gt;Namely, the probability \(P(w_i | c_j)\) is computed by first building
a &lt;a href="https://en.wikipedia.org/wiki/Huffman_coding"&gt;Huffman tree&lt;/a&gt; with words as leaves,
so that most common words are found at smaller depth.
Each internal node \(n_i\) has two children and their associated probabilities \(p_i\) and
\(q_i = 1 - p_i\) where \(p_i\) depends both on \(c_j\) and a vector which is learned during training.
The probaility \(P(w_i | c_j)\) is then computed by following the path from the root to the
\(w_i\) and multiplying the \(p_i\) or \(q_i\) depending on which child is followed.
Since the depth of a balanced tree with \(n\) leaves is \(O(log(n))\), this reduces the computation of
the softmax loss from \(O(W)\) to \(O(log(W)\) for \(W\) words. A more detailed explanation with nice
graphics can be found &lt;a href="http://building-babylon.net/2017/08/01/hierarchical-softmax/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a more rigorous description of negative sampling and hierarchical softmax, see
&lt;a href="https://arxiv.org/abs/1310.4546"&gt;this article&lt;/a&gt; and references therein.
In general, ns or hs are the only way to train the model in reasonable time. As a rule of thumb,
ns requires several epochs to be accurate (-epoch flag) wheread hs does not benefits from more epoch
(&lt;a href="https://groups.google.com/forum/#!msg/word2vec-toolkit/WUWad9fL0jU/LdbWy1jQjUIJ"&gt;as stated here&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The "-bucket" option is used to tune the number of buckets used in the hash table. There is a
trade-off between accuracy and memory used as a smaller value implies more collisions.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The "-lr" option tunes the learning rate. In general, a larger value means faster trainig and a
potentially less accurate model because of a higher risk of getting stuck in local minimum.&lt;/p&gt;
&lt;h1&gt;Some results&lt;/h1&gt;
&lt;p&gt;After a rather light parameter tuning phase, the best cutoff to balance TP and FP results in
a ~2/3 precision. Interestingly, this is almost the best that can be reached.
Indeed, lowering the threshold further (i.e. predicting more and more patients as "pathology") yields
very marginal changes on the precision, TP and FP until reaching a threshold where everything is predicted
as "pathology". This is because most of the low probabilities are all clustered very closeley around the
same value.&lt;/p&gt;
&lt;p&gt;All in all, these results are fairly good considering I did not have to engineer any features and spent
little time optimizing the parameters.
The next step is to see whether word2vec features could benefit a classifier when used along LDA
topic mixtures.&lt;/p&gt;
&lt;h1&gt;Further thoughts:&lt;/h1&gt;
&lt;p&gt;I stumbled upon other alternatives in a similar philosophy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://radimrehurek.com/gensim/models/doc2vec.html"&gt;Gensim's Doc2Vec&lt;/a&gt; does exactly what it
  says: it computes the embedding of whole documents/sentences which can then be fed to a classifier.
  Note: Gensim also implements word2vec and FastText.&lt;/li&gt;
&lt;li&gt;Using word2vec/FasText, compute a component-wise max or min or average over all word representations
  and use the resulting vector as the sentence embedding. Computing all three min and max and average and
  concatenating them can also be used to get another embedding. Idea from this
  &lt;a href="https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector"&gt;stackexchange post&lt;/a&gt;
  and &lt;a href="https://arxiv.org/abs/1607.00570"&gt;this paper&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.01759"&gt;https://arxiv.org/abs/1607.01759&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/fastText/"&gt;https://github.com/facebookresearch/fastText/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/fastText/blob/master/tutorials"&gt;https://github.com/facebookresearch/fastText/blob/master/tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term"&gt;https://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/forum/#!msg/word2vec-toolkit/WUWad9fL0jU/LdbWy1jQjUIJ"&gt;https://groups.google.com/forum/#!msg/word2vec-toolkit/WUWad9fL0jU/LdbWy1jQjUIJ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1310.4546"&gt;https://arxiv.org/abs/1310.4546&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://building-babylon.net/2017/08/01/hierarchical-softmax/"&gt;http://building-babylon.net/2017/08/01/hierarchical-softmax/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Huffman_coding"&gt;https://en.wikipedia.org/wiki/Huffman_coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector"&gt;https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://radimrehurek.com/gensim/models/doc2vec.html"&gt;https://radimrehurek.com/gensim/models/doc2vec.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.00570"&gt;https://arxiv.org/abs/1607.00570&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="NLP"></category><category term="Text classification"></category><category term="Word embedding"></category></entry></feed>