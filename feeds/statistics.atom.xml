<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Weblog - Statistics</title><link href="/" rel="alternate"></link><link href="/feeds/statistics.atom.xml" rel="self"></link><id>/</id><updated>2017-11-25T18:59:00+01:00</updated><entry><title>Maximum likelihood estimation</title><link href="/maximum-likelihood-estimation.html" rel="alternate"></link><published>2017-11-25T18:50:00+01:00</published><updated>2017-11-25T18:50:00+01:00</updated><author><name>Simon-M</name></author><id>tag:None,2017-11-25:/maximum-likelihood-estimation.html</id><summary type="html">&lt;p&gt;Some notes about maximum likelihood estimation&lt;/p&gt;</summary><content type="html">&lt;p&gt;Beside &lt;a href="epidemiology-cheat-sheet.html"&gt;epidemiology&lt;/a&gt;, I am also studying some statistics.
Below you can find my notes on far on maximum likelihood estimation (MLE).&lt;/p&gt;
&lt;p&gt;Let \(X = (X_1, \dots, X_n)\) be a vector of iid random variables following a
distribution \(D_{\theta_0}\) parametrized by the vector \(\theta_0\).
In the continuous case, let \(f_{\theta_0}\) be the probability density function (PDF) of \(D_{\theta_0}\).
In the discrete case, let \(P_{\theta_0}\) be the probability mass function (PMF) of \(D_{\theta_0}\)
Let \(x = (x_1, \dots, x_n)\) be a realization of \(X\), i.e. the &lt;em&gt;observed data&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Then, the maximum likelihood estimator (MLE) \(\hat{\theta}\) of \(\theta_0\) is computed as follows.&lt;/p&gt;
&lt;p&gt;First compute the likelihood \(\mathcal{L}(\theta) = P(X; \theta)\).
If \(F_{\theta}\) is continuous, then
\[\mathcal{L}(\theta) = \prod{i=1}^{n} f_{\theta}(x_i)\]
If \(F_{\theta}\) is discrete, then
\[\mathcal{L}(\theta) = \prod{i=1}^{n} P_{\theta}(x_i)\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt;: the iid assumtion is crucial as it allows the total probability to be equal to the product
of individual probabilities.&lt;/p&gt;
&lt;p&gt;The log-likelihood \(\mathcal{l}(\theta)\) is simply the logarithm of \(\mathcal{L}(\theta)\).
Since \(\log\) is a strictly increasing function, the value maximizing \(\mathcal{l}(\theta)\) is the
same as that maximizing \(\mathcal{L}(\theta)\). The log-likelihood has the added benefit that it
allows to get rid of the product which will prove useful for the next calculations.&lt;/p&gt;
&lt;p&gt;In order to maximize the likelihood, we first need to compute its derivative which respect to the
parameter of interest (namely \(\theta\)), set the derivative to 0 and solve for \(\theta\):
\[u(\theta) = \frac{\partial \mathcal{l}(\theta)}{\partial \theta} = 0\]
This derivative is sometimes called &lt;em&gt;score function&lt;/em&gt; or "Fisher's score function".&lt;/p&gt;
&lt;p&gt;The score is a random vector whose expectation at the true parameter value \(E_{\theta}[u(\theta)_0]\)
is equal to 0.&lt;/p&gt;
&lt;p&gt;The score variance also called &lt;em&gt;information matrix&lt;/em&gt; of "Fisher information matrix" is the positive
semidefinite symmetric matrix:
\[\mathcal{I}(\theta) = var(u(\theta)) =
   E_{\theta} \left[ \left( \frac{\partial \mathcal{l}(\theta)}{\partial \theta} \right)^2 \right] \]
which under mild regularity conditions can be written
\[\mathcal{I}(\theta) =
   -E_{\theta} \left[ \frac{\partial^2 \mathcal{l}(\theta)}{\partial \theta\theta^T} \right] \]&lt;/p&gt;
&lt;p&gt;Asymptotically we have: \( E[\hat{\theta}] = \theta_0\) and
\( \sigma^2(\hat{\theta}) = \mathcal{I}(\theta_0)^{-1})\)
Therefore, \(\hat{\theta}\) is an unbiased estimator for \(\theta_0)\).&lt;/p&gt;
&lt;!--
The Cram√©r-Rao bound gives us a lower bound on the variance of \\(\\hat{\\theta}\\):
\\[var(\\hat{\\theta}) \\ge 1 / \\mathcal{I}(\\theta)\\]
--&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;: The second order derivative of the log-likelihood is a measure of its curvature, i.e. how "sharply peaked" it is.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"&gt;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound"&gt;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Fisher_information"&gt;https://en.wikipedia.org/wiki/Fisher_information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Likelihood_function"&gt;https://en.wikipedia.org/wiki/Likelihood_function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s"&gt;https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Statistics"></category><category term="Maximum likelihood"></category></entry><entry><title>Taking random effects into account: mixed-(effect) models and marginal models.</title><link href="/taking-random-effects-into-account-mixed-effect-models-and-marginal-models.html" rel="alternate"></link><published>2017-10-31T21:30:00+01:00</published><updated>2017-11-25T18:59:00+01:00</updated><author><name>Simon-M</name></author><id>tag:None,2017-10-31:/taking-random-effects-into-account-mixed-effect-models-and-marginal-models.html</id><summary type="html">&lt;p&gt;A review of fixed and random effects, linear mixed models and GEE&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Context&lt;/h1&gt;
&lt;p&gt;I am going to get my hands on some data with correlated measurements and thus have been reading
about to handle such these. Among existing methods are various kinds of ANOVA
(two-way ANOVA, repeated-measures ANOVA, clustered ANOVA, nested/random effects model/hierarchical ANOVA,
split-plot ANOVA), mixed-(effects) models and generalized estimating equations aka marginal
models.&lt;/p&gt;
&lt;p&gt;As of now, mixed models and marginal models are pretty much the standard tools for such analyses,
mainly because ANOVA-based methods require stronger assumptions and do not handle missing data well.
I will almost exclusively focus on &lt;em&gt;linear&lt;/em&gt; models but be aware that teir exist generalized linear mixed models (GLMM) which are the mixed-model version of generalized linear models (GLM). Moreover, generalized stimating equations are an extension of GLM for correlated responses, and as such they can handle the same kind of dependences between covariates and response through the link function.&lt;/p&gt;
&lt;h1&gt;Linear fixed-effects models&lt;/h1&gt;
&lt;p&gt;The response variable \(Y_i\) of observation \(i\) is a random variable which linearly depends
on the covariates \(x_{ij}\) through coefficients \(\beta_j\) up to an random error term \(\epsilon_i\):
\[ Y_i = \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i = \sum_{j=1}^{p} \beta_j x_{ij} + \epsilon_i\]
Equivalently letting \(\mathbf{\beta} = (\beta_1, \dots \beta_p)\) and&lt;br&gt;
\(\mathbf{x_i} = (x_{i1}, \dots x_{ip})^T\) we have:
\[ Y_i = \mathbf{\beta} \mathbf{x_i} + \epsilon_i \]
The assumptions are \(E[\epsilon_i] = 0\), \(Var[\epsilon_i] = \sigma_i^2\), and
\(cov[\epsilon] = \Sigma\).
Most often, \(\epsilon\) is assumed to follow a multivariate Gaussian:
 \(\epsilon \sim \mathcal{N}(0, \Sigma)\).&lt;/p&gt;
&lt;p&gt;Let \(\mathbf{Y}\) be the \(n \times 1\) vector of responses,
\(\mathbf{x}\) be the \(n \times p\) &lt;em&gt;design matrix&lt;/em&gt;, and \(\mathbf{\epsilon}\)
be the \(n \times 1\) vector of random errors. In the more compact matrix notation, we have:
\[ \mathbf{Y} = \mathbf{\beta} \mathbf{x} + \mathbf{\epsilon} \]&lt;/p&gt;
&lt;p&gt;In the fixed-effects model, \(\mathbf{x}\) is assumed to be fixed, and does not contribute to
the variance of \(\mathbf{Y}\):
\[Y \sim \mathcal{N}(\mathbf{\beta} \mathbf{x}, \Sigma)\]&lt;/p&gt;
&lt;h1&gt;Fixed and random effects: an example&lt;/h1&gt;
&lt;p&gt;In the course of an analysis, one may want to include a specific categorical covariate
in the model as a &lt;em&gt;dummy variable&lt;/em&gt;; for instance whether a given patient has diabetes.
In that case, this covariate is relevant to the analysis and not expected to change
should the experiment be carried-out again.&lt;/p&gt;
&lt;p&gt;One may also want to to control for the center (A or B) in which the measurement were made (in a
multi-center study) as it could have an impact on the measurements. However, the effect of the center on the
response is not of primary interest, but should merely be taken into account as a source of heterogeneity
(it is a &lt;em&gt;nuisance&lt;/em&gt; variable). Moreover, the distribution of patients among centers could well change in
another study.&lt;/p&gt;
&lt;p&gt;In the first case there are two groups (diabetes / no diabetes) whose effect on the response is
expected to be accurately quantified by the corresponding dummy variable. In the second case,
there are two groups whose effect on the response can only be estimated given the current data
because another study using other groups will result in different effects.&lt;/p&gt;
&lt;p&gt;The first case is typical of a &lt;em&gt;fixed effect&lt;/em&gt; whereas the second is typical of a &lt;em&gt;random effect&lt;/em&gt;.
Most models including random effects also include fixed effects and can be called &lt;em&gt;random effect models&lt;/em&gt;,
&lt;em&gt;mixed models&lt;/em&gt;, &lt;em&gt;conditional models&lt;/em&gt; \(\dots\).&lt;/p&gt;
&lt;p&gt;In a more general setting, random effects can be either categorical or continuous.&lt;/p&gt;
&lt;h1&gt;Linear mixed effects models (LMM):&lt;/h1&gt;
&lt;p&gt;Linar mixed model include both fixed and random effects which are described separately.
For observation j belonging to group i, we have:
\[ Y_{ij} = \beta_1 x_{ij1} + \dots + \beta_p x_{ijp} + U_{i1} Z_{ij1}  + \dots + U_{iq} Z_{ijq} + \epsilon_{ij}
= \sum_{k=1}^{p} \beta_j x_{ijk} + \sum_{l=1}^{q} U_{il} Z_{ijl} + \epsilon_{ij}\]&lt;/p&gt;
&lt;p&gt;As for the fixed effects model, given and \(\mathbf{U_{i}} = (U_{i1}, \dots, U_{iq})\) and
\(\mathbf{Z_{ij}} = (Z_{ij1}, \dots, Z_{1jq})^T\), this is equivalent to:
\[ Y_{ij} = \mathbf{\beta} \mathbf{x_{ij}} + \mathbf{U_{i}} \mathbf{Z_{ij}} + \epsilon_{ij}\]&lt;/p&gt;
&lt;p&gt;Note that this is essentially the fixed-effects models to which were added with the random effects
\(\mathbf{Z_{ij}}\) and the corresponding group-specific coefficients \(\mathbf{U_{i}}\).&lt;/p&gt;
&lt;p&gt;The equations above assume that all groups have random effects of the
same dimension \(q\). This is not mandatory and it is possible to have
and \(\mathbf{U_{i}} = (U_{i1}, \dots, U_{iq_i})\) and
\(\mathbf{Z_{ij}} = (Z_{ij1}, \dots, Z_{ijq_i})^T\)&lt;/p&gt;
&lt;p&gt;Let \(n_i\) be the number of observations in group \(i\) and \(\mathbf{Z_i}\) be the
\(n_i \times q_i\) design matrix for random effects.
The other terms are similar to those in the fixed-effects model but restricted to group \(i\).
Namely \(\mathbf{x_i}\) is of dimension \(n_i \times p\), and \(\mathbf{\epsilon_i}\)
is of dimension \(n_i \times 1\).
Then a mixed model takes the following form for group \(i\):
\[ \mathbf{Y_i} = \mathbf{\beta} \mathbf{x_i} + \mathbf{U_i} \mathbf{Z_i} + \mathbf{\epsilon_i}\]&lt;/p&gt;
&lt;p&gt;Finally, letting \(n = \sum_{i} n_i\) and \(q = \sum_{i} q_i\) we have
\[ \mathbf{Y} = \mathbf{\beta} \mathbf{x} + \mathbf{U} \mathbf{Z} + \mathbf{\epsilon}\]
with \(\mathbf{Z}\) of dimension \(n \times q\) and  \(\mathbf{U}\) of dimension \(q \times 1\).&lt;/p&gt;
&lt;h2&gt;Variance components&lt;/h2&gt;
&lt;p&gt;As opposed to the fixed effects model, in this setting \(\epsilon\) is no longer the only contributor to
the variance of \(\mathbf{Y}\).
We have \(U \sim \mathcal{N}(0, R)\), \(R\) being the random effects covariance matrix, and
\(\epsilon\) and \(U\) are assumed to be independent, i.e. \(cov[U, \epsilon] = 0_{q \times n}\).&lt;/p&gt;
&lt;p&gt;Therefore, we have
\(\mathbf{Y|U} \sim \mathcal{N}(\mathbf{X}\mathbf{\beta} + \mathbf{Z}\mathbf{U}, \Sigma\))
and
\(\mathbf{Y} \sim \mathcal{N}(\mathbf{X}\mathbf{\beta}, ZRZ^T + \Sigma) =
\mathcal{N}(\mathbf{X}\mathbf{\beta}, V)\).
The matrix \(V\) is called the &lt;em&gt;variance-covariance matrix&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;Model fitting / Parameter estimation&lt;/h1&gt;
&lt;p&gt;For a mixed model, the unknown quantities \(\mathbf{\beta}\), \(\mathbf{U}\), \(\Sigma\) and \(R\)
must be estimated.
More specifically, \(\mathbf{\beta}\), \(\Sigma\) and \(R\) are fixed and which can be directly
estimated from the data. The situation is rather different for \(\mathbf{U}\) which is a matrix of
random variables which must be predicted.&lt;/p&gt;
&lt;p&gt;I'll first assume that the covariance matrices are known to estimate \(\mathbf{\beta}\) and \(\mathbf{U}\),
which is almost never the case but makes the explanation simpler. In practice these and the covariance matrices are
estimated jointly.
In practice, parameter estimation is usually performed using either maximum likelihood (ML) or
residual/restricted maximum likelihood (RML / REML).&lt;/p&gt;
&lt;h2&gt;Random and fixed effects estimation&lt;/h2&gt;
&lt;p&gt;Let us start by assuming that \(V = ZRZ^T + \Sigma\) is known, implying that  \(R\) and
\(\Sigma\) are known too, and seek to estimate \(\mathbf{\beta}\) and \(\mathbf{U}\).
One way is to solve Henderson's equations also called &lt;em&gt;mixed model equations&lt;/em&gt; with the
following solutions:
\[ \mathbf{\hat{\beta}} = (\mathbf{X}^T V^{-1} \mathbf{X})^{-1} X^T V^{-1} \mathbf{Y}\]
\[ \mathbf{\tilde{U}} = R Z^T V^{-1}(\mathbf{Y} - \mathbf{X}\mathbf{\hat{\beta}})\]&lt;/p&gt;
&lt;p&gt;Note that \(\mathbf{\hat{\beta}}\) is a essentially a least squares estimate
(recall the usual estimate \((X^T X) - X^T Y\)) weighted by the inverse of the variance-covariance
matrix. It is also called &lt;em&gt;generalized least squares estimate&lt;/em&gt; and is unbiased:
\(E[\mathbf{\hat{\beta}}] = \mathbf{\beta}\).&lt;/p&gt;
&lt;p&gt;The estimate \(\mathbf{\tilde{U}}\) is the best linear unbiased predictor (BLUP) of
\(\mathbf{U}\) and as such, we have \(E[\mathbf{\tilde{U}}] = \mathbf{U}\).
Here &lt;em&gt;best&lt;/em&gt; refers to the fact that it os the most &lt;em&gt;efficient&lt;/em&gt;, i.e. has the minimum variance, among
unbiased predictors. Note how it depends on the residuals \(\mathbf{Y} - \mathbf{X}\mathbf{\hat{\beta}}\)
of the "fixed effect model".&lt;/p&gt;
&lt;h2&gt;Variance parameter estimation&lt;/h2&gt;
&lt;p&gt;For the previous estimations to be computed, one needs to first estimate \(V\) and
thus \(R\) and \(\Sigma\). "First" here is misleading however as the estimation of
effects and variance parameters are usually done jointly.&lt;/p&gt;
&lt;p&gt;The experimental design can sometimes give some information about the structure of these matrices and
one can specify their &lt;em&gt;structure&lt;/em&gt; prior to estimation.
For instance knowing that the random effects are independent from each other, one may enforce a
diagonal structure for \(R\).
Similarly for the random errors \(\Sigma\), one may, for instance, assume that all observations are
equally correlated (compound symmetry) or that their correlations get weaker with their distance
in the dataset (e.g. autoregressive structure for longitudinal data where obervations are sorted
by time). Virtually any correlation structure can be specified but only the most commonly used are
supported by statistical softwares, which is most of the time enough for any type of analysis.&lt;/p&gt;
&lt;h2&gt;Maximum likelihood&lt;/h2&gt;
&lt;p&gt;Let \(\theta\) be the vector containing unknown parameters in \(R\) and \(\Sigma\). Their number
depends on the correlatino structure chosen: more constraints lead to less parameters to estimate.
To derivate the likelihood of the model, recall that
\(\mathbf{Y} \sim \mathcal{N}(\mathbf{X}\mathbf{\beta}, V(\theta))\). Thus the log-likelihood
\(l(\mathbf{Y}; \mathbf{\beta}, V(\theta))\) of \(\mathbf{Y}\) satisfies:
\[-2 l(\mathbf{Y}; \mathbf{\beta}, V(\theta)) = \log |V(\theta)| +
    (\mathbf{Y} - \mathbf{X}\mathbf{\beta})^T V(\theta)^{-1}(\mathbf{Y} - \mathbf{X}\mathbf{\beta}) + n \log 2 \pi\]&lt;/p&gt;
&lt;p&gt;Minimizing this log-likelihood can be done by replacing \(\mathbf{\beta}\) by its maximum
likelihood estimator
\(\mathbf{\hat{\beta}}(\theta) = \mathbf{X}^T V(\theta)^{-1} \mathbf{X})^{-1} X^T V(\theta)^{-1} \mathbf{Y}\)
(starting with e.g. random intial parameters \(\theta\)) and minimizing
\(l(\mathbf{Y}; \mathbf{\hat{\beta}}(\theta), V(\theta))\) with respect to \(\theta\).&lt;/p&gt;
&lt;p&gt;The main caveat of the ML estimation is that the variance estimates are negatively (downward) biased
as they do not take into account the degrees of freedom lost while estimating the fixed effects.
Indeed, \(\mathbf{\beta}\) is estimated alongside \(V(\theta)\) (i.e. it is not known without
uncertainty), and this is not taken into account in the number of degrees of freedom (\(n\) should be
replaced by a smaller value).&lt;/p&gt;
&lt;h2&gt;Restricted maximum likelihood&lt;/h2&gt;
&lt;p&gt;Restricted maximum likelihood estimation addresses the biasedness of MLE estimators.
The core idea is to perform MLE on a modified version of the data, namely linear combinations of \(\mathbf{Y}\).&lt;/p&gt;
&lt;p&gt;First, notice that the design matrix \(\mathbf{X}\) has rank at most \(p\) since it is a \( n \times p\)
matrix.
Let \(r = rank(\mathbf{X})\) and \(K\) be the full rank \((n - r) \times n\) matrix satisfying
\(E[K\mathbf{Y}] = 0 \iff E(K\mathbf{X}\mathbf{\beta}) = 0\).
Thus, \(K\mathbf{Y} \sim \mathcal{N}(0, K V(\theta) K^T)\) and maximum likelihood
estimation can be carried out on \(K\mathbf{Y}\) with \((n - r)\) degrees of freedom.
Moreover, notice that as opposed to \(\mathbf{Y} (\sim \mathcal{N}(\mathbf{X}\mathbf{\beta}, V)\),
\(\mathbf{\beta}\) no longer appear in the distribution of \(K\mathbf{Y}\), meaning that the
dependence on fixed effects has been removed.
MOre conretely, the transformed response \(K\mathbf{Y}\) consists of the residuals obtained after
fitting the fixed effects ignoring the variance parameters.&lt;/p&gt;
&lt;p&gt;In contrast with ML, the REML estimates for fixed effect are biased and those for random effect
are unbiased.&lt;/p&gt;
&lt;p&gt;Remark: REML can also be described from a marginal likelihood point of vue. The marginal likelihood
\(L_R(\mathbf{Y}; V(\theta))\) with respect to \(\mathbf{Y}\) is found by integrating out the fixed
effects:
\(L_R(\mathbf{Y}; V(\theta)) = \int L(\mathbf{Y}; \mathbf{\beta}, V(\theta)) d \mathbf{\beta}\).
Then log of the marginal likelihood is then maximized.&lt;/p&gt;
&lt;p&gt;In practice, ML and REML are typically computed using iterative schemes such as the Newton-Raphson
algorithm and variants thereof, or the expectation-maximization algorithm.&lt;/p&gt;
&lt;h1&gt;Marginal models and Generalized estimating equations&lt;/h1&gt;
&lt;p&gt;As an alternative to LMM, generalized estimating equations (GEE) are often used for modeling correlated
data. In that context they commonly appear under the name &lt;em&gt;marginal models&lt;/em&gt;. Marginal models are actually
a subset of the more general GEE called population-average GEE (PA-GEE)  as opposed to subject-specific
GEE (SS-GEE).&lt;/p&gt;
&lt;h2&gt;GEE are an extension of generalized linear models (GLM).&lt;/h2&gt;
&lt;p&gt;Let us forget about random effects for a moment.
The difference between fixed-effects linear models and GLM is that for the latter, the response variable
is no longer restricted to be normally distributed. Instead, it can follow any distribution from the
exponential family.
Moreover, a &lt;em&gt;link&lt;/em&gt; function \(g\) describes how the expectation \(\mu_i\) of \(Y_i\) depends
on a linear combination of covariates.
This results in a model of the follwing form:
\[ g(E[Y_i]) = \beta_1 x_{i1} + \dots + \beta_p x_{ip}= \mathbf{x_i} \mathbf{\beta}\]
or in compact form:
\[ g(E[Y]) = \mathbf{X}\mathbf{\beta} \]&lt;/p&gt;
&lt;p&gt;For instance, linear regression consists of a normally distributed \(Y\) with a identity
link function, and logistic regression consists of a binomially distributed \(Y\) with a logit
\((x \mapsto \ log \frac{x}{1-x}) \) link function.&lt;/p&gt;
&lt;p&gt;GEE extend GLM by allowing correlated \(Y_i\). The covariance structure can be specified as for
mixed models. However, note that the covariance is no longer between random effects or random errors
but between responses.
As such, the formulation of GEE does not explicitely include random effects.
In the linear case (\(g\) is the identify function), the model is therefore:
\[ Y_i = \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i = \mathbf{x_i} \mathbf{\beta} + \epsilon_i \]
or in compact form
\[ Y = \mathbf{\beta} \mathbf{x} + \epsilon \]
where \(\epsilon \sim \mathcal{N}(0, \Sigma)\).
In that case, there is no \(R\) matrix and  \(V = \Sigma\) is the variance-covariance matrix of
the model.
Note that no assumption is made about the distribution of \(Y\) as opposed to LMM.&lt;/p&gt;
&lt;h2&gt;Parameter estimation&lt;/h2&gt;
&lt;p&gt;The parameter estimation of marginal models relies on &lt;em&gt;quasi-likelihood&lt;/em&gt; instead of the
(restricted)-likelihood. It does not require the full distribution of \(Y\) to be known
but only its mean and covariance matrix.
In practice, this amounts to solving the following &lt;em&gt;quasi-likelihood estimating equation&lt;/em&gt;
(see &lt;a href="https://onlinecourses.science.psu.edu/stat504/node/182"&gt;here&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Generalized_estimating_equation"&gt;here&lt;/a&gt;):
\[X^T V^{-1}(Y - \mathbf{X} \mathbf{\beta)} = 0\]&lt;/p&gt;
&lt;p&gt;However, note that the covariance matrix \(V\) is unknown at this point, thus it is replaced by
an estimate \(\tilde{V}\) which depends on \(\beta\) and initially assumes independence
(\(\tilde{V}\) is diagonal).
The quasi-likelihood estimating equation is then solved iteratively, alternating between the
estimation of \(\mathbf{\beta}\) with fixed \(\tilde{V}\) and of \(\tilde{V}\) with fixed
\(\mathbf{\beta}\).&lt;/p&gt;
&lt;p&gt;A very rough description of the algorithm is therefore:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Fit an intial fixed effect model without taking into account any covariance
  (assume independence, diagonal \(V^0\)), yielding \(\beta^0\)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At step \(i\), update the covariance matrix \(V^i\) using the current
   estimates \(\beta^{i-1}\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update \(\beta^i\) from the covariance matrix \(V^i\).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Iterate 2. and 3. until convergence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;More details can be found
&lt;a href="http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details01.htm"&gt;here&lt;/a&gt;,
&lt;a href="http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details06.htm"&gt;here&lt;/a&gt; and
&lt;a href="https://www.ibm.com/support/knowledgecenter/de/SSLVMB_20.0.0/com.ibm.spss.statistics.help/alg_genlin_gee_estimation_param.htm"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The estimation of the variance of \(\beta\) on the other hand uses the so-called
&lt;em&gt;robust sandwich estimator&lt;/em&gt; (some nice and short explanations in the context of OLS regression
&lt;a href="http://thestatsgeek.com/2013/10/12/the-robust-sandwich-variance-estimator-for-linear-regression/"&gt;here&lt;/a&gt;
and &lt;a href="https://stats.stackexchange.com/questions/50778/sandwich-estimator-intuition"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h1&gt;Mixed-effect models versus maringla models&lt;/h1&gt;
&lt;p&gt;For linear models, mixed and marginal models yield the same estimates.
This is however not true in the general case.&lt;/p&gt;
&lt;h2&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;Mixed-models aim at estimating group-specific coefficients i.e. they condition on both the design matrix and
random effects:
\[E[Y | \mathbf{U}] = \mathbf{x} \mathbf{\beta} + \mathbf{Z} \mathbf{U}\]
In other words, they describe how the response of individual groups changes with
the covariates. The random effects allow the covariate coefficients to vary randomly from one group
to another, thereby providing a group-specific response. For this reason they are sometimes called
&lt;em&gt;conditional models&lt;/em&gt; as opposed to marginal models.&lt;/p&gt;
&lt;p&gt;On the other hand, marginal models aim at estimating population average coefficients.
Indeed, as their name suggests, they focuses on marginals, i.e. take averages over random effects,
only conditioning on the design matrix:
\[E[Y] = \mathbf{x} \mathbf{\beta} \]
In other words, marginal models focus on the impact of covariates on the response over the whole population.&lt;/p&gt;
&lt;p&gt;In terms of interpretation of the coefficients, a very clear example is provided
&lt;a href="https://stats.stackexchange.com/questions/86309/marginal-model-versus-random-effects-model-how-to-choose-between-them-an-advi"&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;"&lt;em&gt;If you are a doctor and you want an estimate of how much a statin drug will lower your
patient‚Äôs odds of getting a heart attack, the subject-specific coefficient
is the clear choice. On the other hand, if you are a state health official and you want to
know how the number of people who die of heart attacks would change if everyone in the at-risk population
took the stain drug, you would probably want to use the population‚Äìaveraged coefficients. (Allison, 2009)&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Where "subject-specific" refers to the mixed effect / conditional model whereas "population-averaged" refers
to the GEE / marginal model.&lt;/p&gt;
&lt;h2&gt;Assumptions and robustness&lt;/h2&gt;
&lt;p&gt;As opposed to LMM, marginal models do not rely on the assumption that random effects are
normally distributed.&lt;/p&gt;
&lt;p&gt;Marginal models are more robust than LMM when the covariance structure is misspecified, a nice
feature of the sandwich estimator.&lt;/p&gt;
&lt;p&gt;Marginal models can only handle data missing completely at random (MCAR, "really random") wheread LMM
can also handle data missing at random (MAR, randomness with dependence on the covariates).&lt;/p&gt;
&lt;p&gt;Choosing between miwed models and marginal models is therefore primarily a question of scope (group-specific
versus population-wise) and also depend on what is known about the data (how certain are we about the correlation
structure? and about the complete randomness of missing data?).&lt;/p&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Campbell, Michael J. Statistics at square two: understanding modern statistical applications in medicine. BMJ, 2001&lt;/li&gt;
&lt;li&gt;Duchateau, Luc, Paul Janssen, and John Rowlands. Linear mixed models. An introduction with applications in veterinary research. ILRI (aka ILCA and ILRAD), 1998&lt;/li&gt;
&lt;li&gt;Burton, Paul, Lyle Gurrin, and Peter Sly. "Extending the simple linear regression model to account for correlated responses: an introduction to generalized estimating equations and multilevel mixed modelling." (2004): 1-33&lt;/li&gt;
&lt;li&gt;Hardin, James W. Generalized estimating equations (GEE). John Wiley &amp;amp; Sons, Ltd, 2005&lt;/li&gt;
&lt;li&gt;Naseri, Parisa, et al. "Comparison of generalized estimating equations (GEE), mixed effects models (MEM) and repeated measures ANOVA in analysis of menorrhagia data." Journal of Paramedical Sciences 7.1 (2016): 32-40&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://statistics.ma.tum.de/fileadmin/w00bdb/www/czado/lec10.pdf"&gt;http://statistics.ma.tum.de/fileadmin/w00bdb/www/czado/lec10.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.musc.edu/simhille/Presentations/GEE_tutorial_Betsy/GEE_Tutorial.pdf"&gt;http://people.musc.edu/simhille/Presentations/GEE_tutorial_Betsy/GEE_Tutorial.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.misug.org/uploads/8/1/9/1/8191072/kwelch_repeated_measures.pdf"&gt;http://www.misug.org/uploads/8/1/9/1/8191072/kwelch_repeated_measures.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf"&gt;http://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.cmu.edu/~hseltman/309/Book/chapter15.pdf"&gt;http://www.stat.cmu.edu/~hseltman/309/Book/chapter15.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/02-21.pdf"&gt;http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/02-21.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/03-23.pdf"&gt;http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/03-23.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/04-06.pdf"&gt;http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/04-06.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/04-18.pdf"&gt;http://www.stat.ncsu.edu/people/bloomfield/courses/ST732/04-18.pdf}&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://onlinecourses.science.psu.edu/stat504/node/180"&gt;https://onlinecourses.science.psu.edu/stat504/node/180&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_genmod_details29.htm"&gt;https://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_genmod_details29.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details01.htm"&gt;http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details01.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details06.htm"&gt;http://support.sas.com/documentation/cdl/en/statug/67523/HTML/default/viewer.htm#statug_gee_details06.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ibm.com/support/knowledgecenter/de/SSLVMB_20.0.0/com.ibm.spss.statistics.help/alg_genlin_gee_estimation_param.htm"&gt;https://www.ibm.com/support/knowledgecenter/de/SSLVMB_20.0.0/com.ibm.spss.statistics.help/alg_genlin_gee_estimation_param.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Generalized_estimating_equation"&gt;https://en.wikipedia.org/wiki/Generalized_estimating_equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model"&gt;https://en.wikipedia.org/wiki/Generalized_linear_model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://thestatsgeek.com/2013/10/12/the-robust-sandwich-variance-estimator-for-linear-regression/"&gt;http://thestatsgeek.com/2013/10/12/the-robust-sandwich-variance-estimator-for-linear-regression/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm"&gt;https://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/48671/what-is-restricted-maximum-likelihood-and-when-should-it-be-used"&gt;https://stats.stackexchange.com/questions/48671/what-is-restricted-maximum-likelihood-and-when-should-it-be-used&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model/68753#68753"&gt;https://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model/68753#68753&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/86309/marginal-model-versus-random-effects-model-how-to-choose-between-them-an-advi"&gt;https://stats.stackexchange.com/questions/86309/marginal-model-versus-random-effects-model-how-to-choose-between-them-an-advi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/62923/models-for-generalized-estimating-equation"&gt;https://stats.stackexchange.com/questions/62923/models-for-generalized-estimating-equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/62939/gee-quasi-likelihood-and-what-it-generalizes"&gt;https://stats.stackexchange.com/questions/62939/gee-quasi-likelihood-and-what-it-generalizes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stats.stackexchange.com/questions/50778/sandwich-estimator-intuition"&gt;https://stats.stackexchange.com/questions/50778/sandwich-estimator-intuition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/statistics/comments/16k9z6/can_anyone_help_me_understand_when_to_use/"&gt;https://www.reddit.com/r/statistics/comments/16k9z6/can_anyone_help_me_understand_when_to_use/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="Statistics"></category><category term="Mixed models"></category><category term="Generalized estimating equations"></category><category term="Marginal models"></category><category term="Conditional models"></category><category term="Random effects"></category></entry></feed>