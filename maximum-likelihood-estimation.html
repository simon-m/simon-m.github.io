<!DOCTYPE html>
<html lang="en">
<head>
          <title>Weblog</title>
        <meta charset="utf-8" />
        <link href="https://simon-m.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Weblog Full Atom Feed" />
        <link href="https://simon-m.github.io/feeds/statistics.atom.xml" type="application/atom+xml" rel="alternate" title="Weblog Categories Atom Feed" />


    <meta name="tags" content="Statistics" />
    <meta name="tags" content="Maximum likelihood" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://simon-m.github.io/">Weblog <strong>Random notes about data analysis at large</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="categories.html">Categories</a></li>
            <li><a href="tags.html">Tags</a></li>
            <li><a href="https://simon-m.github.io/pages/links.html">Links</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://simon-m.github.io/maximum-likelihood-estimation.html" rel="bookmark"
         title="Permalink to Maximum likelihood estimation">Maximum likelihood estimation</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2017-11-25T18:50:00+01:00">
      Sat 25 November 2017
    </time>
    <time class="modified" datetime="2017-11-25T18:50:00+01:00">
      Sat 25 November 2017
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://simon-m.github.io/author/simon-m.html">Simon-M</a>
    </address>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p>Beside <a href="epidemiology-cheat-sheet.html">epidemiology</a>, I am also studying some statistics.
Below you can find my notes on far on maximum likelihood estimation (MLE).</p>
<p>Let \(X = (X_1, \dots, X_n)\) be a vector of iid random variables following a
distribution \(D_{\theta_0}\) parametrized by the vector \(\theta_0\).
In the continuous case, let \(f_{\theta_0}\) be the probability density function (PDF) of \(D_{\theta_0}\).
In the discrete case, let \(P_{\theta_0}\) be the probability mass function (PMF) of \(D_{\theta_0}\)
Let \(x = (x_1, \dots, x_n)\) be a realization of \(X\), i.e. the <em>observed data</em>.</p>
<p>Then, the maximum likelihood estimator (MLE) \(\hat{\theta}\) of \(\theta_0\) is computed as follows.</p>
<p>First compute the likelihood \(\mathcal{L}(\theta) = P(X; \theta)\).
If \(F_{\theta}\) is continuous, then
\[\mathcal{L}(\theta) = \prod{i=1}^{n} f_{\theta}(x_i)\]
If \(F_{\theta}\) is discrete, then
\[\mathcal{L}(\theta) = \prod{i=1}^{n} P_{\theta}(x_i)\]</p>
<p><strong>NB</strong>: the iid assumtion is crucial as it allows the total probability to be equal to the product
of individual probabilities.</p>
<p>The log-likelihood \(\mathcal{l}(\theta)\) is simply the logarithm of \(\mathcal{L}(\theta)\).
Since \(\log\) is a strictly increasing function, the value maximizing \(\mathcal{l}(\theta)\) is the
same as that maximizing \(\mathcal{L}(\theta)\). The log-likelihood has the added benefit that it
allows to get rid of the product which will prove useful for the next calculations.</p>
<p>In order to maximize the likelihood, we first need to compute its derivative which respect to the
parameter of interest (namely \(\theta\)), set the derivative to 0 and solve for \(\theta\):
\[u(\theta) = \frac{\partial \mathcal{l}(\theta)}{\partial \theta} = 0\]
This derivative is sometimes called <em>score function</em> or "Fisher's score function".</p>
<p>The score is a random vector whose expectation at the true parameter value \(E_{\theta}[u(\theta)_0]\)
is equal to 0.</p>
<p>The score variance also called <em>information matrix</em> of "Fisher information matrix" is the positive
semidefinite symmetric matrix:
\[\mathcal{I}(\theta) = var(u(\theta)) =
   E_{\theta} \left[ \left( \frac{\partial \mathcal{l}(\theta)}{\partial \theta} \right)^2 \right] \]
which under mild regularity conditions can be written
\[\mathcal{I}(\theta) =
   -E_{\theta} \left[ \frac{\partial^2 \mathcal{l}(\theta)}{\partial \theta\theta^T} \right] \]</p>
<p>Asymptotically we have: \( E[\hat{\theta}] = \theta_0\) and
\( \sigma^2(\hat{\theta}) = \mathcal{I}(\theta_0)^{-1})\)
Therefore, \(\hat{\theta}\) is an unbiased estimator for \(\theta_0)\).</p>
<!--
The CramÃ©r-Rao bound gives us a lower bound on the variance of \\(\\hat{\\theta}\\):
\\[var(\\hat{\\theta}) \\ge 1 / \\mathcal{I}(\\theta)\\]
-->

<p><strong>Remark</strong>: The second order derivative of the log-likelihood is a measure of its curvature, i.e. how "sharply peaked" it is.</p>
<h1>References</h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">https://en.wikipedia.org/wiki/Maximum_likelihood_estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fisher_information">https://en.wikipedia.org/wiki/Fisher_information</a></li>
<li><a href="https://en.wikipedia.org/wiki/Likelihood_function">https://en.wikipedia.org/wiki/Likelihood_function</a></li>
<li><a href="https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s">https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s</a></li>
</ul>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>